{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Text Processing\n",
    "Natural Language Processing - Universidad Tecnológica Nacional"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a word vectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming a list of strings to a bag of words and vectorizing them\n",
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['disk' 'format' 'hard' 'how' 'my' 'problems' 'to']\n",
      "\n",
      "Ocurrences' vectors of each word:\n",
      "[[1 1 1 1 1 0 1]\n",
      " [1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "bow = vectorizer.get_feature_names_out()\n",
    "ocurrences_vectors = X.toarray()\n",
    "\n",
    "print(f'Bag of words:\\n{bow}')\n",
    "print(f\"\\nOcurrences' vectors of each word:\\n{ocurrences_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocurrences of the word 'hard in the second document: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ocurrences of the word 'hard in the second document: {ocurrences_vectors[1,2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with a real documents collection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the categories\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', \n",
    "'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the trainig dataset by categories\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "shuffle=True, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing the training data\n",
    "vectorizer = CountVectorizer()\n",
    "train_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the word 'algorithm': 4690\n"
     ]
    }
   ],
   "source": [
    "freq_algorithm = vectorizer.vocabulary_.get('algorithm')\n",
    "print(f\"Frequency of the word 'algorithm': {freq_algorithm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of tokens extracted: 35788\n"
     ]
    }
   ],
   "source": [
    "#Amount of tokens extracted\n",
    "num_tokens = len(vectorizer.get_feature_names_out())\n",
    "print(f\"Amount of tokens extracted: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a word vectorizer without stop words\n",
    "vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a stemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of 'cats': cat\n",
      "Stem of 'loving': love\n"
     ]
    }
   ],
   "source": [
    "print(f\"Stem of 'cats': {stemmer.stem('cats')}\")\n",
    "print(f\"Stem of 'loving': {stemmer.stem('loving')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn CountVectorizer with NLTK stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_analyze = \"John bought carrots and potatoes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a build analyzer without a stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definig a build analyzer\n",
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'John bought carrots and potatoes' analyzed without stemming:\n",
      " ['john', 'bought', 'carrots', 'potatoes']\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{text_to_analyze}' analyzed without stemming:\\n {analyze(text_to_analyze)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a build analyzer with a stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an english stemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining an stemmer build analyzer class\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the class before defined\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a build analyzer with stemming\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'John bought carrots and potatoes' analyzed with stemming:\n",
      " ['john', 'bought', 'carrot', 'potato']\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{text_to_analyze}' analyzed with stemming:\\n {stem_analyze(text_to_analyze)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results on 'John bought carrots and potatoes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'John bought carrots and potatoes' analyzed without stemming:\n",
      " ['john', 'bought', 'carrots', 'potatoes']\n",
      "\n",
      "'John bought carrots and potatoes' analyzed with stemming:\n",
      " ['john', 'bought', 'carrot', 'potato']\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{text_to_analyze}' analyzed without stemming:\\n {analyze(text_to_analyze)}\\n\")\n",
    "print(f\"'{text_to_analyze}' analyzed with stemming:\\n {stem_analyze(text_to_analyze)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking results using **20_Newsgroups**' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words extracted with stemming: 26888\n"
     ]
    }
   ],
   "source": [
    "#Amount of tokens extracted using stemming\n",
    "train_counts = stem_vectorizer.fit_transform(twenty_train.data) \n",
    "print(f\"Number of words extracted with stemming: {len(stem_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words extracted with stemming: 35482\n"
     ]
    }
   ],
   "source": [
    "#Amount of tokens extracted not using stemming\n",
    "train_counts = vectorizer.fit_transform(twenty_train.data) \n",
    "print(f\"Number of words extracted with stemming: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "#Importing a spanish corpus\n",
    "from nltk.corpus import cess_esp\n",
    "print(cess_esp.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "#Importing spanish stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_esp = stopwords.words('spanish')\n",
    "print(stopwords_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a spanish stemmer\n",
    "spanish_stemmer = nltk.stem.SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_CountVectorizer = CountVectorizer(min_df=1, stop_words=stopwords_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a spanish vectorizer with stemmer class\n",
    "class SpanishStemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SpanishStemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([spanish_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the class before defined\n",
    "spanish_stem_vectorizer = SpanishStemmedCountVectorizer(min_df=1, stop_words=stopwords_esp)\n",
    "\n",
    "#Instantiating the spanish build analyzer\n",
    "spanish_stem_analyze = spanish_stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del 51_por_ciento de la empresa mexicana Electricidad_Águila_de_Altamira -Fpa- EAA -Fpt- , creada por el japonés Mitsubishi_Corporation para poner_en_marcha una central de gas de 495 megavatios .' analyzed with a stemmer:\n",
      "['grup', 'estatal', 'electricite_de_franc', 'fpa', 'edf', 'fpt', 'anunc', 'hoy', 'juev', 'compr', '51_por_cient', 'empres', 'mexican', 'electricidad_aguila_de_altamir', 'fpa', 'eaa', 'fpt', 'cre', 'japones', 'mitsubishi_corporation', 'poner_en_march', 'central', 'gas', '495', 'megavati']\n",
      "\n",
      "'Una portavoz de EDF explicó a EFE que el proyecto para la construcción de Altamira_2 , al norte de Tampico , prevé la utilización de gas natural como combustible principal en una central de ciclo combinado que debe empezar a funcionar en mayo_del_2002 .' analyzed without a stemmer:\n",
      "['portavoz', 'edf', 'explic', 'efe', 'proyect', 'construccion', 'altamira_2', 'nort', 'tampic', 'prev', 'utiliz', 'gas', 'natural', 'combust', 'principal', 'central', 'cicl', 'combin', 'deb', 'empez', 'funcion', 'mayo_del_2002']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing the first and second sentence of the corpus\n",
    "first_sentence = ' '.join(cess_esp.sents()[0])\n",
    "second_sentence = ' '.join(cess_esp.sents()[1])\n",
    "\n",
    "print(f\"'{first_sentence}' analyzed with a stemmer:\\n{spanish_stem_analyze(first_sentence)}\\n\")\n",
    "print(f\"'{second_sentence}' analyzed without a stemmer:\\n{spanish_stem_analyze(second_sentence)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

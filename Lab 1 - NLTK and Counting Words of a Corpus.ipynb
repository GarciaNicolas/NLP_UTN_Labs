{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - NLTK and Counting Words of a Corpus\n",
    "Natural Language Processing - Universidad Tecnol√≥gica Nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.book import *\n",
    "texts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Moby', 'Dick', 'by', 'Herman']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 5 tokens from text1\n",
    "text1.tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 5 tokens from text7\n",
    "text7.tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count ocurrences of \"Moby\" in text1\n",
    "text1.count(\"Moby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
      "Number of elements: 7\n"
     ]
    }
   ],
   "source": [
    "#Tokenize a sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "mysent = \"The cat sat on the mat.\"\n",
    "mysent_token = word_tokenize(mysent)\n",
    "\n",
    "print(f\"Tokens: {mysent_token}\\nNumber of elements: {len(mysent_token)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of m: <re.Match object; span=(8, 12), match='lang'> \n",
      "Start: 8 \n",
      "End: 12\n"
     ]
    }
   ],
   "source": [
    "from nltk import re\n",
    "\n",
    "#Looking for \"lang\" into \"natural language processing\" \n",
    "m = re.search(\"lang\",\"natural language processing\")\n",
    "print(f\"Type of m: {m} \\nStart: {m.start()} \\nEnd: {m.end()}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only returns a coincidence objecte if there was a match for the regular expression in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Number of elements: 6      \n",
      "Set: {'mat', 'on', 'The', 'cat', 'the', 'sat'}\n",
      "Number of elements in the set: 6\n"
     ]
    }
   ],
   "source": [
    "#Alfanumeric tokens only\n",
    "mysent_tokens_nopunct = [word for word in word_tokenize(mysent) if re.search(\"\\w\",word)]\n",
    "print(f\"Tokens: {mysent_tokens_nopunct}\\nNumber of elements: {len(mysent_tokens_nopunct)}\\\n",
    "      \\nSet: {set(mysent_tokens_nopunct)}\\nNumber of elements in the set: {len(set(mysent_tokens_nopunct))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of tokens, we are keeping only those were the word match with the expression \\w.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center; font-style: italic;\"> \\w is equals to [a-zA-Z0-9_] </p>\n",
    "\n",
    "Besides that, the set of tokens is repeating the token \"The\" because is key sensitive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Number of elements: 6      \n",
      "Set: {'mat', 'on', 'cat', 'the', 'sat'}\n",
      "Number of elements in the set: 5\n"
     ]
    }
   ],
   "source": [
    "#Lowering the sentence.\n",
    "mysent_tokens_nopunct_lower = [word for word in word_tokenize(mysent.lower()) if re.search(\"\\w\",word)]\n",
    "print(f\"Tokens: {mysent_tokens_nopunct_lower}\\nNumber of elements: {len(mysent_tokens_nopunct_lower)}\\\n",
    "      \\nSet: {set(mysent_tokens_nopunct_lower)}\\nNumber of elements in the set: {len(set(mysent_tokens_nopunct_lower))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moby', 'dick', 'by', 'herman', 'melville']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying techniques to a bigger corpus\n",
    "moby_dick_tokens = text1.tokens\n",
    "moby_dick_nopunct = [word.lower() for word in moby_dick_tokens if re.search('\\w',word)]\n",
    "moby_dick_nopunct[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to remove punctuations and transform the tokens to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_dick_tokens = text1.tokens\n",
    "moby_dick_tokens_nopunct_lower = [word.lower() for word in moby_dick_tokens if re.search('\\w',word)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many tokens are in Moby Dick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 218621\n"
     ]
    }
   ],
   "source": [
    "moby_dick_number_tokens = len(moby_dick_tokens_nopunct_lower)\n",
    "print(f\"Number of tokens: {moby_dick_number_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many types are in Moby Dick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 17140\n"
     ]
    }
   ],
   "source": [
    "moby_dick_number_types = len(set(moby_dick_tokens_nopunct_lower))\n",
    "print(f\"Number of tokens: {moby_dick_number_types}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate Moby Dick type-token ratio *(lexical diversity)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07840051962071347\n"
     ]
    }
   ],
   "source": [
    "moby_dick_type_token_ratio = moby_dick_number_types / moby_dick_number_tokens\n",
    "\n",
    "print(moby_dick_type_token_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate Wall Street Journal type-token ratio *(lexical diversity)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.129748424801388\n"
     ]
    }
   ],
   "source": [
    "wsj_tokens = text7.tokens\n",
    "wsj_tokens_nopunct_lower = [word.lower() for word in wsj_tokens if re.search('\\w', word)]\n",
    "\n",
    "wsj_type_token_ratio = len(set(wsj_tokens_nopunct_lower)) / len(wsj_tokens_nopunct_lower)\n",
    "\n",
    "print(wsj_type_token_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which one has more lexical diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal has 0.051347905180674544 more lexical diversity ratio than Moby Dick\n"
     ]
    }
   ],
   "source": [
    "print(f\"Wall Street Journal has {wsj_type_token_ratio - moby_dick_type_token_ratio} more lexical diversity ratio than Moby Dick\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Could you think a reason of why one is more diverse than the other one?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is because the Wall Street Journal have differents authors and topics for each new, creating a bigger diversity of words than Moby Dick, which is a story told by its only author is Herman Melville."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which is the Maximum Likelihood Estimate (MLE) of the word \"Whale\" in Moby Dick? *P_mobydick(\"whale\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_mobydick(\"whale\")= 0.005607878474620462\n"
     ]
    }
   ],
   "source": [
    "moby_dick_mle_whale = moby_dick_tokens_nopunct_lower.count(\"whale\") / len(moby_dick_tokens_nopunct_lower)\n",
    "print(f\"P_mobydick(\\\"whale\\\")= {moby_dick_mle_whale}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which is the Maximum Likelihood Estimate (MLE) of the word \"Whale\" in Wall Street Journal? *P_wsj(\"whale\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_wsj(\"whale\")= 0.0\n"
     ]
    }
   ],
   "source": [
    "wsj_mle_whale = wsj_tokens_nopunct_lower.count(\"whale\") / len(wsj_tokens_nopunct_lower)\n",
    "print(f\"P_wsj(\\\"whale\\\")= {wsj_mle_whale}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-utn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
